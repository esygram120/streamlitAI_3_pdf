# Streamlit + Google Gemini API + LangChain

import streamlit as st
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain_google_genai import ChatGoogleGenerativeAI

with st.sidebar:
    "[í™ˆìœ¼ë¡œ ëŒì•„ê°€ê¸°](http://localhost:8080/)"
    gemini_api_key = st.text_input("Gemini API Key", key="chatbot_api_key", type="password")
    uploaded_file = st.file_uploader("PDF íŒŒì¼ ì—…ë¡œë“œ", type=("pdf"))
    process = st.button("ë­ì²´ì¸ ì—°ë™í•˜ê¸°")

if process:
    # session_state ì´ˆê¸°í™”
    if "retriever" not in st.session_state:
        st.session_state.retriever = None

    # PDF íŒŒì¼ ì²˜ë¦¬
    file_name = uploaded_file.name
    with open(file_name, "wb") as file:
        file.write(uploaded_file.getvalue())

    # PDF ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    loader = PyPDFLoader(file_name)
    pages = loader.load_and_split()

    # PDF ë‚´ìš©ì„ ì‘ì€ chunk ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸°
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)
    splits = text_splitter.split_documents(pages)

    # ì„ë² ë”© ëª¨ë¸ ì„¤ì •
    model_name = "jhgan/ko-sroberta-multitask" # (KorNLU ë°ì´í„°ì…‹ì— í•™ìŠµì‹œí‚¨ í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸)
    model_kwargs = {'device': 'cpu'}
    encode_kwargs = {'normalize_embeddings': False}
    embedding_model = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )

    # ChromaDB ì €ì¥ & Retriever ì„¤ì •
    vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)
    vectorstore_retriever = vectorstore.as_retriever()

    st.session_state.retriever = vectorstore_retriever


st.title("ğŸš€ PDF íŒŒì¼ì²˜ë¦¬ ì„œë¹„ìŠ¤ ğŸ’¬")
st.caption("Streamlit + Google Gemini API + LangChain ë¥¼ ì—°ë™í–ˆìŠµë‹ˆë‹¤.")
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•œ í›„, PDFì—ì„œ ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt := st.chat_input():
    if not gemini_api_key:
        st.info("Google Gemini API í‚¤ë¥¼ ì…ë ¥í•˜ê³  ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•´ì£¼ì„¸ìš”.")
        st.stop()

    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # LLM ì„¤ì •
    llm = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0, google_api_key=gemini_api_key)
    
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
    template = """ë‹¤ìŒê³¼ ê°™ì€ ë§¥ë½ì„ ì‚¬ìš©í•˜ì—¬ ë§ˆì§€ë§‰ ì§ˆë¬¸ì— ëŒ€ë‹µí•˜ì‹­ì‹œì˜¤.
    # ë‹µë³€ì€ ìµœëŒ€ ì„¸ ë¬¸ì¥ìœ¼ë¡œ í•˜ê³  ê°€ëŠ¥í•œ í•œ ê°„ê²°í•˜ê²Œ ìœ ì§€í•˜ì‹­ì‹œì˜¤.
    # {context}
    # ì§ˆë¬¸: {question}
    # ë„ì›€ì´ ë˜ëŠ” ë‹µë³€:"""
    rag_prompt_custom = PromptTemplate.from_template(template)

    # RAG chain ì„¤ì •
    rag_chain = {"context": st.session_state.retriever, "question": RunnablePassthrough()} | rag_prompt_custom | llm
    response = rag_chain.invoke(f'{prompt}')
    msg = response.content
    st.session_state.messages.append({"role": "assistant", "content": msg})
    st.chat_message("assistant").write(msg)